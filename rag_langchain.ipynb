{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GENERATE_MODEL_NAME = os.getenv(\"GENERATE_MODEL_NAME\")\n",
    "EMBEDDINGS_MODEL_NAME = os.getenv(\"EMBEDDINGS_MODEL_NAME\")\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "NGROK_STATIC_DOMAIN = os.getenv(\"NGROK_STATIC_DOMAIN\")\n",
    "NGROK_TOKEN = os.getenv(\"NGROK_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer directly (tokyotech-llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923b1dc7457d4be49b244200cbc76289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "def embedding_model(model_choice: str) -> object:\n",
    "    if model_choice == \"llama3\":\n",
    "        embedding = OllamaEmbeddings(model=model_choice)\n",
    "    elif model_choice == \"Sentence Transformers\":\n",
    "        embedding = HuggingFaceInferenceAPIEmbeddings(\n",
    "            model_name=EMBEDDINGS_MODEL_NAME,\n",
    "            api_key=HUGGINGFACE_API_KEY)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Web URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "#from fake_useragent import UserAgent\n",
    "\n",
    "#os.environ['USER_AGENT'] = UserAgent().chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Content from HTMl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl Web data use recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_web(url_data):\n",
    "    loader = RecursiveUrlLoader(url=url_data, extractor=bs4_extractor, max_depth=4)\n",
    "    docs = loader.load()\n",
    "    print('length: ', len(docs))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=500)\n",
    "    all_splitts = text_splitter.split_documents(docs)\n",
    "    print('length_all_splits: ', len(all_splitts))\n",
    "    return all_splitts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from single URL (no recursion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_base_loader(url_data):\n",
    "    loader = WebBaseLoader(url_data)\n",
    "    docs = loader.load()\n",
    "    print('length: ', len(docs))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    all_splitts = text_splitter.split_documents(docs)\n",
    "    return all_splitts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(documments, filename, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    data_to_save = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documments]\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data_to_save, file, indent=4)\n",
    "    print(f'Data saved to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_local(filename: str, directory: str) -> tuple:\n",
    "    file_path = os.path.join(directory, file_path)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(f'Data loaded from {file_path}')\n",
    "    \n",
    "    return data, filename.rsplit('.', 1)[0].replace('_', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to the Qdrant Vector DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_to_qdrant(URI_link: str, collection_name: str) -> Qdrant:\n",
    "def connect_to_qdrant(model_name: str, url: str, api: str, collection_name: str) -> Qdrant:\n",
    "    embedding = embedding_model(model_name)\n",
    "    client = QdrantClient(url=url,\n",
    "                          api_key=api,\n",
    "                          prefer_grpc=False)\n",
    "    db = Qdrant(client=client,\n",
    "                embeddings=embedding,\n",
    "                collection_name=collection_name)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vector_em(URI_link: str, collection_name: str, filename: str, directory: str, use_ollama: bool = False) -> Qdrant:\n",
    "\n",
    "def vector_em(collection_name: str, filename: str, directory: str, use_ollama: bool = False) -> Qdrant:\n",
    "    if use_ollama: \n",
    "        model_name = \"llama3\"\n",
    "    else:\n",
    "        model_name = \"Sentence Transformers\"\n",
    "        \n",
    "    local_data, doc_name = load_data_from_local(filename, directory)\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=doc.get('page_content') or '',\n",
    "            metadata={\n",
    "                'source': doc['metadata'].get('source') or '',\n",
    "                'content_type': doc['metadata'].get('content_type') or 'text/plain',\n",
    "                'title': doc['metadata'].get('title') or '',\n",
    "                'description': doc['metadata'].get('description') or '',\n",
    "                'language': doc['metadata'].get('language') or 'en',\n",
    "                'doc_name': doc_name,\n",
    "                'start_index': doc['metadata'].get('start_index') or 0\n",
    "            }\n",
    "        )\n",
    "        for doc in local_data\n",
    "    ]\n",
    "    print('documents: ', documents)\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    vectorstore = connect_to_qdrant(model_name=model_name, \n",
    "                                    url=QDRANT_URL,\n",
    "                                    api=QDRANT_API_KEY, \n",
    "                                    collection_name=QDRANT_COLLECTION_NAME)\n",
    "    vectorstore.add_documents(documents=documents, ids=uuids)\n",
    "    print('vector: ', vectorstore)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qdrant Live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qdrant_live(URL: str, URI_link: str, collection_name: str, doc_name: str, use_ollama: bool = False) -> Qdrant:\n",
    "\n",
    "def qdrant_live(URL: str, collection_name: str, doc_name: str, use_ollama: bool = False) -> Qdrant:\n",
    "    if use_ollama: \n",
    "        model_name = \"llama3\"\n",
    "    else:\n",
    "        model_name = \"Sentence Transformers\"\n",
    "    documents = craw_web(URL)\n",
    "    for doc in documents:\n",
    "        metadata={\n",
    "            'source': doc['metadata'].get('source') or '',\n",
    "            'content_type': doc['metadata'].get('content_type') or 'text/plain',\n",
    "            'title': doc['metadata'].get('title') or '',\n",
    "            'description': doc['metadata'].get('description') or '',\n",
    "            'language': doc['metadata'].get('language') or 'en',\n",
    "            'doc_name': doc_name,\n",
    "            'start_index': doc['metadata'].get('start_index') or 0\n",
    "        }\n",
    "        doc.metadata = metadata\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    vectorstore = connect_to_qdrant(model_name=model_name, \n",
    "                                    url=QDRANT_URL,\n",
    "                                    api=QDRANT_API_KEY, \n",
    "                                    collection_name=QDRANT_COLLECTION_NAME)\n",
    "    vectorstore.add_documents(documents=documents, ids=uuids)\n",
    "    print('vector: ', vectorstore)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from typing import List\n",
    "from langchain_community.retrievers import BM25Retriever # Retriever base on BM25\n",
    "from collections import defaultdict\n",
    "#from langchain.retrievers import EnsembleRetriever # Combine lost of retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RerankRetriever(AutoTokenizer)\n",
    "Can then re-rank the top candidates from the first stage for higher accuracy in critical cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/d3h7vx393175sc9kvpd02znh0000gn/T/ipykernel_27084/3137967646.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class RerankRetriever(VectorStoreRetriever):\n"
     ]
    }
   ],
   "source": [
    "class RerankRetriever(VectorStoreRetriever):\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]: # Return the similar document\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "        candidates = [doc.page_content for doc in docs]\n",
    "        queries = [query]*len(candidates)\n",
    "        features = tokenizer(queries, candidates, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model(**features).logits\n",
    "            values, indices = torch.sum(scores, dim=1).sort(descending=True)\n",
    "            #relevant_docs = doc[indices[0]]\n",
    "        return [docs[indices[0]], docs[indices[1]]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RerankWikiRetriever(AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/d3h7vx393175sc9kvpd02znh0000gn/T/ipykernel_27084/247739537.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class RerankWikiRetriever(VectorStoreRetriever):\n"
     ]
    }
   ],
   "source": [
    "class RerankWikiRetriever(VectorStoreRetriever):\n",
    "    vectorstore: WikipediaRetriever\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]: # Return the similar document\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "        candidates = [doc.page_content for doc in docs]\n",
    "        queries = [query]*len(candidates)\n",
    "        features = tokenizer(queries, candidates, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model(**features).logits\n",
    "            values, indices = torch.sum(scores, dim=1).sort(descending=True)\n",
    "            #relevant_docs = docs[indices[0]]\n",
    "        return [docs[indices[0]], docs[indices[1]]]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrievre(Embedding)\n",
    "Can be used as a first-stage retrieval system to fetch a larger set of documents quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/d3h7vx393175sc9kvpd02znh0000gn/T/ipykernel_27084/4224465620.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class get_retriever(VectorStoreRetriever):\n"
     ]
    }
   ],
   "source": [
    "class get_retriever(VectorStoreRetriever):\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    def __init__(self, qdrant_weight=0.7, bm25_weight=0.3):\n",
    "        self.weights = [qdrant_weight, bm25_weight]\n",
    "    def get_relevant_documents(self, query: str, k: int = 4) -> List[Document]:\n",
    "        qdrant_retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs = {\"k\": k}\n",
    "        )\n",
    "        qdrant_docs = self.vectorstore.get_relevant_documents(query)\n",
    "        documents = [\n",
    "            Document(page_content = doc.page_content, metadata = doc.metadata)\n",
    "            for doc in self.vectorstore.similarity_search(\"\", k = 100)\n",
    "        ]\n",
    "        if not documents:\n",
    "            raise ValueError(f\"Not found documents in the collection '{QDRANT_COLLECTION_NAME}'\")\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = k\n",
    "        bm25_docs = bm25_retriever.get_relevant_documents(query)\n",
    "        socred_docs = self.combine_and_socre_documents(qdrant_docs, bm25_docs)   \n",
    "        return socred_docs[:k]\n",
    "    \n",
    "    def combine_and_score_documents(self, qdrant_docs: List[Document], bm25_docs: List[Document]) -> List[Document]:\n",
    "        doc_scores = defaultdict(float) \n",
    "        for i, doc in enumerate(qdrant_docs):\n",
    "            doc_scores[doc] += (self.weights[0] * (1 / (i + 1)))\n",
    "        for i, doc in enumerate(bm25_docs):\n",
    "            doc_scores[doc] += (self.weights[1] * (1 / (i + 1)))        \n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in sorted_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLM Sever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, MultiRetrievalQAChain\n",
    "from transformers import pipeline\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "import torch\n",
    "#from langchain.llms import VLLM\n",
    "#from vllm import LLM, SamplingParams\n",
    "#from langchain.llms import HuggingFaceHub\n",
    "\n",
    "class LLMServe:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = self.load_embeddings()\n",
    "        self.current_source = \"wiki\"\n",
    "        self.retriever = self.load_retriever(retriever_name=self.current_source, embeddings=self.embeddings)\n",
    "        self.pipe = self.load_model_pipeline(max_new_tokens=300)\n",
    "        self.prompt = self.load_prompt_template()\n",
    "        self.rag_pipeline = self.load_rag_pipeline(llm=self.pipe, retriever=self.retriever, prompt=self.prompt)\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "            model_name=EMBEDDINGS_MODEL_NAME,\n",
    "            api_key=HUGGINGFACE_API_KEY,\n",
    "        )\n",
    "        return embeddings\n",
    "    \n",
    "    def load_retriever(self, retriever_name, embeddings):\n",
    "        retriever=None\n",
    "        if retriever_name == \"wiki\":\n",
    "            retriever = RerankWikiRetriever(vectorstore=WikipediaRetriever(lang=\"jp\", doc_content_chars_max=800, top_k_results=15))\n",
    "        #elif retriever_name == \"combine\":\n",
    "            #retriever = get_retriever(vectorstore=db)\n",
    "        else:\n",
    "            client = QdrantClient(url=QDRANT_URL,\n",
    "                              api_key=QDRANT_API_KEY,\n",
    "                              prefer_grpc=False)\n",
    "            db = Qdrant(client=client, \n",
    "                    embeddings=embeddings,\n",
    "                    collection_name=QDRANT_COLLECTION_NAME)\n",
    "            retriever = RerankRetriever(vectorstore=db.as_retriever(search_kwargs={\"k\": 15}))\n",
    "        return retriever\n",
    "    \n",
    "    def load_model_pipeline(self, max_new_tokens=300):\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=GENERATE_MODEL_NAME,\n",
    "            device= device,\n",
    "            max_new_tokens=max_new_tokens)\n",
    "        return pipe\n",
    "    \n",
    "    def load_prompt_template(self):\n",
    "        # query_template = \"あなたは東京工業大学のアシスタントです。以下の質問に文脈をもとに回答してください。もし文脈が答えを提供していない、または確実でない場合は、『この情報は分かりませんが、参考文献の情報が役立つかもしれません！』と答えてください。文脈にない情報を作り出さないでください。\\n文脈: {context} \\n質問: {question}\\n回答: \"\n",
    "        # query_template = \"文脈を参照してください:{context}\\n\\n### 質問:{question}\\n\\n### 回答:\"\n",
    "        query_template = (\n",
    "            \"あなたは文脈(context)に基づいて質問に答える賢いチャットボットです。\\n\\n\"\n",
    "            \"### 文脈: {context} \\n\\n### 人間: {question}\\n\\n### アシスタント:\")\n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template, \n",
    "            input_variables=[\"context\", \"question\"])\n",
    "        return prompt\n",
    "    \n",
    "    def load_rag_pipeline(self, pipe, retriever, prompt):\n",
    "        rag_pipeline = RetrievalQA.from_chain_type(\n",
    "            llm=HuggingFacePipeline(pipeline=pipe),\n",
    "            chain_type='stuff',\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True)\n",
    "        return rag_pipeline\n",
    "    \n",
    "    def rag(self, source):\n",
    "        if source == self.current_source:\n",
    "            return self.rag_pipeline\n",
    "        else:\n",
    "            self.retriever = self.load_retriever(retriever_name=source, embeddings=self.embeddings)\n",
    "            self.rag_pipeline = self.load_rag_pipeline(llm=self.pipe, retriever=self.retriever, prompt=self.prompt)\n",
    "            self.current_source = source\n",
    "            return self.rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95215426dfc49cfa0ded0b9408acb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31m詳細については、Jupyter [ログ] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "app = LLMServe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
